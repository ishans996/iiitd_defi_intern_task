{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13a7d03b",
   "metadata": {},
   "source": [
    "## [TODO]\n",
    "\n",
    "- If overfit, then Dropout \n",
    "- Should glove embeddings be updated?\n",
    "- Dealing with unknown tokens\n",
    "- kl lambda annealing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3426ab5",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e4aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.distributions as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fab6af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba2f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be78b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed_size = 100\n",
    "embedder_hidden_size = 100\n",
    "gru_num_layers = 1\n",
    "window_size = 4\n",
    "\n",
    "dropout_vmd_in = 0.3\n",
    "vmd_hidden_size = 150\n",
    "g_size = 50\n",
    "y_size = 2\n",
    "\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064e957d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.state = State()\n",
    "        self.Embedder = Embedder()\n",
    "        self.VMD = VMD(self.state)\n",
    "        self.ATA = ATA(self.state)\n",
    "        \n",
    "    \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        X = self.Embedder(X)\n",
    "        self.VMD(X, Y)\n",
    "        self.ATA(Y)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872983a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def get_z_and_z_distr(self, arg, is_prior):\n",
    "        mean = nn.Linear(arg.size(-1), vmd_hidden_size)(arg)\n",
    "        stddev = nn.Linear(arg.size(-1), vmd_hidden_size)(arg)      \n",
    "        stddev = torch.sqrt(torch.exp(stddev))\n",
    "        epsilon = torch.randn(vmd_hidden_size)\n",
    "        \n",
    "        z = mean if is_prior else mean + torch.mul(epsilon, stddev)\n",
    "        z_pdf = ds.normal.Normal(loc=mean, scale=stddev)\n",
    "        return z, z_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae890b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.bi_gru = nn.GRU(word_embed_size, embedder_hidden_size, num_layers=gru_num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            X: window_size (for speaker's history) * max_words_in_a_speech * input_size\n",
    "            \n",
    "        \"\"\"\n",
    "#         X = get_glove_padded_seq(X)\n",
    "        h_0 = torch.randn(2*gru_num_layers, window_size, embedder_hidden_size) # multiplied by 2 because bidirectional\n",
    "        _, h_n = self.bi_gru(X, h_0)\n",
    "        h_f = h_n[0, :, :] \n",
    "        h_b = h_n[1, :, :]\n",
    "        msg_embed = (h_f + h_b) / 2\n",
    "        return msg_embed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9050969e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMD(nn.Module):\n",
    "    def __init__(self, state):\n",
    "        super(VMD, self).__init__()\n",
    "#         self.input_dropout = nn.Dropout(p=dropout_vmd_in)\n",
    "        self.state = state    \n",
    "        self.gru = nn.GRU(embedder_hidden_size, vmd_hidden_size, num_layers=gru_num_layers, bidirectional=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "#         X = self.input_dropout(X)\n",
    "        h_0 = torch.randn(gru_num_layers, vmd_hidden_size) \n",
    "        h_s, _ = self.gru(X, h_0) # h_s: window_size * vmd_hidden_size\n",
    "        \n",
    "        z_prior = []\n",
    "        z_post = []\n",
    "        kl = []\n",
    "        z_post_t_minus_1 = torch.randn(vmd_hidden_size)\n",
    "        for t in range(window_size):\n",
    "            h_z_prior_t = nn.Linear(embedder_hidden_size+2*vmd_hidden_size, vmd_hidden_size)(\n",
    "                torch.cat([X[t], h_s[t], z_post_t_minus_1])\n",
    "            )\n",
    "            h_z_prior_t = nn.Tanh()(h_z_prior_t)\n",
    "\n",
    "            h_z_post_t = nn.Linear(embedder_hidden_size+2*vmd_hidden_size+y_size, vmd_hidden_size)(\n",
    "                torch.cat([X[t], h_s[t], Y[t], z_post_t_minus_1])\n",
    "            )\n",
    "            h_z_post_t = nn.Tanh()(h_z_post_t)\n",
    "            \n",
    "            z_prior_t, z_prior_t_pdf = self.state.get_z_and_z_distr(h_z_prior_t, is_prior=True)\n",
    "            z_post_t, z_post_t_pdf = self.state.get_z_and_z_distr(h_z_post_t, is_prior=False)\n",
    "            z_post_t_minus_1 = z_post_t\n",
    "            \n",
    "            kl_t = ds.kl.kl_divergence(z_prior_t_pdf, z_post_t_pdf)\n",
    "            \n",
    "            z_prior.append(z_prior_t)\n",
    "            z_post.append(z_post_t)\n",
    "            kl.append(kl_t)\n",
    "        \n",
    "        z_prior = torch.stack(z_prior) # window_size * vmd_hidden_size\n",
    "        z_post = torch.stack(z_post) # window_size * vmd_hidden_size\n",
    "        self.state.kl = torch.stack(kl).sum(dim=1) # window_size\n",
    "        \n",
    "        self.state.g = nn.Linear(2*vmd_hidden_size, g_size)(\n",
    "            torch.cat([h_s, z_post],dim=1) # TODO: check if X is also to be concatenated as in Eqn 21\n",
    "        )        \n",
    "        self.state.g = nn.Tanh()(self.state.g) #\n",
    "        \n",
    "        y = nn.Linear(g_size, y_size)(self.state.g)\n",
    "        self.state.y = nn.Softmax(dim=1)(y)\n",
    "#         print(\"y:\", self.state.y)\n",
    "\n",
    "        self.state.g_T = self.state.g[-1, :] \n",
    "        # TODO: 1. will g_T be different during training and evaluation?\n",
    "        # TODO: 2. is g_T definitely what you think it is?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622283d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d77781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATA(nn.Module):\n",
    "    def __init__(self, state):\n",
    "        super(ATA, self).__init__()\n",
    "        self.state = state\n",
    "        self.alpha = 0.5\n",
    "        \n",
    "        \n",
    "    def forward(self, Y):\n",
    "        linear_i = nn.Linear(g_size, g_size, bias=False)(self.state.g)\n",
    "        linear_i = nn.Tanh()(linear_i) # (window_size, g_size)\n",
    "        w_i = nn.init.xavier_normal_(torch.zeros((g_size, 1))) # (g_size, 1)\n",
    "        v_i = linear_i @ w_i # (window_size, 1)\n",
    "        \n",
    "        linear_d = nn.Linear(g_size, g_size, bias=False)(self.state.g)\n",
    "        linear_d = nn.Tanh()(linear_d)\n",
    "        g_T = self.state.g_T[:, None]\n",
    "        v_d = linear_d @ g_T\n",
    "        \n",
    "        aux_score = torch.mul(v_i, v_d)\n",
    "        aux_score[-1, :] = np.NINF\n",
    "        \n",
    "        v_starred = nn.Softmax(dim=0)(aux_score)\n",
    "        self.state.v_starred = torch.where(torch.isnan(v_starred), torch.tensor(0, dtype=torch.float32), v_starred)\n",
    "        \n",
    "        att_c = self.state.v_starred.T @ self.state.y\n",
    "#         print(att_c.shape, self.state.g_T.shape)\n",
    "        self.state.y_T = nn.Linear(y_size+g_size, y_size)(torch.cat([torch.squeeze(att_c), self.state.g_T]))\n",
    "        self.state.y_T = nn.Softmax(dim=0)(self.state.y_T)  \n",
    "#         print(\"y_T:\", self.state.y_T.shape)\n",
    "        \n",
    "        self.calculate_loss(Y)\n",
    "        \n",
    "        \n",
    "    def calculate_loss(self, Y):\n",
    "#         print(self.state.v_starred)\n",
    "        v_aux = self.alpha * self.state.v_starred\n",
    "        likelihood_aux = torch.sum(torch.mul(Y, torch.log(self.state.y)), dim=1)\n",
    "        \n",
    "        kl_lambda = self.get_kl_lambda()\n",
    "        obj_aux = likelihood_aux - kl_lambda * self.state.kl\n",
    "        \n",
    "        y_T_orig = Y[-1, :]\n",
    "        likelihood_T = torch.sum(torch.mul(y_T_orig, torch.log(self.state.y_T)))\n",
    "        \n",
    "        kl_T = self.state.kl[-1]\n",
    "        obj_T = likelihood_T - kl_lambda * kl_T\n",
    "        \n",
    "        self.state.loss = obj_T + torch.sum(torch.mul(obj_aux, v_aux))\n",
    "        \n",
    "        \n",
    "    def get_kl_lambda(self):\n",
    "        # TODO: implement KL annealing\n",
    "        return 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45245fda",
   "metadata": {},
   "source": [
    "# Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5715be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset/ParlVote/ParlVote_concat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b892ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speakers[speaker_id: int] = list of tuples (debate_id, speech, vote)\n",
    "speakers = defaultdict(lambda: [])\n",
    "for idx, row in data.iterrows():\n",
    "    speakers[row[\"speaker_id\"]].append([row[\"debate_id\"], row[\"speech\"], row[\"vote\"]])\n",
    "        \n",
    "for k, v in speakers.items():\n",
    "    speakers[k] = sorted(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4617a73",
   "metadata": {},
   "source": [
    "## Analysis of speaker sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9398c45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = 0\n",
    "for usid in data.speaker_id.unique():\n",
    "    mx = max(mx, len(speakers[usid]))\n",
    "print(mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0597e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_history_lens = []\n",
    "for sid, each in speakers.items():\n",
    "    speaker_history_lens.append(len(each)) \n",
    "speaker_history_lens.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a7184",
   "metadata": {},
   "outputs": [],
   "source": [
    "shl = pd.Series(speaker_history_lens)\n",
    "shl.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f924402",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(list(filter(lambda x: x >= 4, speaker_history_lens))) / len(speaker_history_lens)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7216198",
   "metadata": {},
   "source": [
    "- The above cell shows 90% speakers have at least *window_size*=4 speeches\n",
    "- If we use all data, we need to pad speeches that have (< *window_size*=4) speeches\n",
    "- Decision: Drop these 10% speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f2b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = dict(filter(lambda val: len(val[1]) >= 4, speakers.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f175e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * (len(speakers) / len(speaker_history_lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255807cd",
   "metadata": {},
   "source": [
    "## Download Glove-100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8eb9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -r -nc https://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffff045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40877a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! head -n1 glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea83d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = dict()\n",
    "for line in open(\"nlp.stanford.edu/data/glove.6B.100d.txt\", \"r\"):\n",
    "    line = line.split()\n",
    "    embeds[line[0]] = torch.tensor(list(map(float, line[1:])), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_vocab = set()\n",
    "# for f in open(\"nlp.stanford.edu/data/glove.6B.100d.txt\", \"r\"):\n",
    "#     glove_vocab.add(f.split()[0])\n",
    "# print(len(glove_vocab))\n",
    "\n",
    "# del glove_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d109f51",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02855a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfbc26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acbb079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s: str):\n",
    "    \n",
    "    # Lowercasing\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    ttk = TweetTokenizer()\n",
    "    tokens = ttk.tokenize(s)\n",
    "#     tokens = word_tokenize(s)\n",
    "    \n",
    "    # Stopwords removal\n",
    "    tokens = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    \n",
    "#     # Stemming\n",
    "#     ps = nltk.PorterStemmer()\n",
    "#     tokens = [ps.stem(w) for w in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060902c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# embed_keys = set(embeds.keys())\n",
    "# ans = 0\n",
    "# i = 0\n",
    "# sids = speakers.keys()\n",
    "# for sid in sids:\n",
    "    \n",
    "#     s = speakers[sid][0][1]\n",
    "#     pre = preprocess(s)\n",
    "#     ans += len(set(pre).difference((set(pre).intersection(embed_keys))))\n",
    "    \n",
    "#     print(\"i\", i, \" done:\", ans)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62947c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds['abhorrent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e25ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input1 = embeds[\"drank\"]\n",
    "# input2 = embeds[\"drunk\"]\n",
    "# cos = nn.CosineSimilarity(dim=0)\n",
    "# output = cos(input1, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381ddabd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9e728a4",
   "metadata": {},
   "source": [
    "## Create PaddedTensor for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70325698",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbb4ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y_padded_tensors(debates):\n",
    "        \"\"\"\n",
    "            X: window_size * max_seq_len * word_embed_size \n",
    "            Y: window_size\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        Y = []\n",
    "        max_tokens = 0\n",
    "        for _, speech, vote in debates:\n",
    "            tokens = preprocess(speech)\n",
    "#             print(\"len preprocessed speech:\", len(tokens))\n",
    "            X.append(torch.stack([embeds.get(token, torch.randn(word_embed_size)) for token in tokens]))\n",
    "            y = torch.zeros(2)\n",
    "            y[vote] = 1. # y = [0, 1] if vote=1 else [1, 0]\n",
    "            Y.append(y)\n",
    "        \n",
    "        X = pad_sequence(X, batch_first=True)\n",
    "        return X, torch.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a521f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a39d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = [len(v) for k, v in speakers.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5ef43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ = 0\n",
    "ans = 0\n",
    "freq_sum = torch.tensor(freq).sum().item()\n",
    "for i in range(len(freq)-1, -1, -1):\n",
    "    if sum_ + (freq[i]) > 0.2 * freq_sum:\n",
    "        break\n",
    "    sum_ += (freq[i])\n",
    "    ans = i\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f98856",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "for epoch in range(num_epochs):\n",
    "    scnt = 0\n",
    "    done_cnt = 0\n",
    "    loss_batch = 0\n",
    "    save_cnt = 0\n",
    "    for speaker_num, (speaker_id, debates) in enumerate(speakers.items()):\n",
    "        for i in range(window_size, len(debates)+1):\n",
    "            X, Y = get_X_Y_padded_tensors(debates[i-window_size: i]) \n",
    "            model(X, Y)\n",
    "            loss_batch += model.state.loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            model.state.loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            done_cnt += 1\n",
    "            if done_cnt%20==0:\n",
    "                print(\"Speaker num:\", speaker_num, \"\\t\", \"done:\", done_cnt, \"\\t\", \"perc:\", 100*(done_cnt/(0.8*freq_sum)), \"\\tloss:\", loss_batch, \"\\n\"+\"-\"*50+\"\\n\")\n",
    "                loss_batch = 0\n",
    "                \n",
    "            if done_cnt%1000 == 0:\n",
    "                torch.save(model.state_dict(), f\"saved_models/final_model_lr_001_kll_0_1_{save_cnt}.pt\")\n",
    "                save_cnt += 1\n",
    "\n",
    "        if scnt > ans:\n",
    "            break\n",
    "        scnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1c2095",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
