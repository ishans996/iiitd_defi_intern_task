{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b75e30",
   "metadata": {},
   "source": [
    "## [TODO]\n",
    "\n",
    "- Dropout everywhere\n",
    "- Should glove embeddings be updated?\n",
    "- Dealing with unknown tokens\n",
    "- You and stocknet-code have included y_T in calculation in ATA, while paper equations have not. Think about it\n",
    "- Correct y_size to 2 from 1 because two classes *aye*, *no*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ece8c5c",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256c1743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch; torch.manual_seed(0)\n",
    "import torch.nn as nn\n",
    "import torch.distributions as ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81641800",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53758be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578fba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed_size = 100\n",
    "embedder_hidden_size = 100\n",
    "gru_num_layers = 1\n",
    "window_size = 4\n",
    "\n",
    "dropout_vmd_in = 0.3\n",
    "vmd_hidden_size = 150\n",
    "g_size = 50\n",
    "y_size = 2\n",
    "\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db737f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.Embedder = Embedder()\n",
    "        self.VMD = VMD(self)\n",
    "        self.ATA = ATA(self)\n",
    "    \n",
    "    \n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        X = self.Embedder(X)\n",
    "        self.VMD(X, Y)\n",
    "        self.ATA(Y)\n",
    "#         print(\"X\", X.shape)\n",
    "#         print(\"kl\", self.kl.shape)\n",
    "#         print(\"g\", self.g.shape)\n",
    "#         print(\"y\", self.y.shape)\n",
    "        # result = something\n",
    "#         return X  \n",
    "    \n",
    "    \n",
    "    def get_z_and_z_distr(self, arg, is_prior):\n",
    "        mean = nn.Linear(arg.size(-1), vmd_hidden_size)(arg)\n",
    "        stddev = nn.Linear(arg.size(-1), vmd_hidden_size)(arg)      \n",
    "        stddev = torch.sqrt(torch.exp(stddev))\n",
    "        epsilon = torch.randn(vmd_hidden_size)\n",
    "        \n",
    "        z = mean if is_prior else mean + torch.mul(epsilon, stddev)\n",
    "        z_pdf = ds.normal.Normal(loc=mean, scale=stddev)\n",
    "        return z, z_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56590d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.bi_gru = nn.GRU(word_embed_size, embedder_hidden_size, num_layers=gru_num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            X: window_size (for speaker's history) * max_words_in_a_speech * input_size\n",
    "            \n",
    "        \"\"\"\n",
    "#         X = get_glove_padded_seq(X)\n",
    "        h_0 = torch.randn(2*gru_num_layers, window_size, embedder_hidden_size) # multiplied by 2 because bidirectional\n",
    "        _, h_n = self.bi_gru(X, h_0)\n",
    "        h_f = h_n[0, :, :] \n",
    "        h_b = h_n[1, :, :]\n",
    "        msg_embed = (h_f + h_b) / 2\n",
    "        return msg_embed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5cf5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VMD(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(VMD, self).__init__()\n",
    "#         self.input_dropout = nn.Dropout(p=dropout_vmd_in)\n",
    "        self.model = model    \n",
    "        self.gru = nn.GRU(embedder_hidden_size, vmd_hidden_size, num_layers=gru_num_layers, bidirectional=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, X, Y):\n",
    "#         X = self.input_dropout(X)\n",
    "        h_0 = torch.randn(gru_num_layers, vmd_hidden_size) \n",
    "        h_s, _ = self.gru(X, h_0) # h_s: window_size * vmd_hidden_size\n",
    "        \n",
    "        z_prior = []\n",
    "        z_post = []\n",
    "        kl = []\n",
    "        z_post_t_minus_1 = torch.randn(vmd_hidden_size)\n",
    "        for t in range(window_size):\n",
    "            h_z_prior_t = nn.Linear(embedder_hidden_size+2*vmd_hidden_size, vmd_hidden_size)(\n",
    "                torch.cat([X[t], h_s[t], z_post_t_minus_1])\n",
    "            )\n",
    "            h_z_prior_t = nn.Tanh()(h_z_prior_t)\n",
    "\n",
    "            h_z_post_t = nn.Linear(embedder_hidden_size+2*vmd_hidden_size+y_size, vmd_hidden_size)(\n",
    "                torch.cat([X[t], h_s[t], Y[t], z_post_t_minus_1])\n",
    "            )\n",
    "            h_z_post_t = nn.Tanh()(h_z_post_t)\n",
    "            \n",
    "            z_prior_t, z_prior_t_pdf = self.model.get_z_and_z_distr(h_z_prior_t, is_prior=True)\n",
    "            z_post_t, z_post_t_pdf = self.model.get_z_and_z_distr(h_z_post_t, is_prior=False)\n",
    "            z_post_t_minus_1 = z_post_t\n",
    "            \n",
    "            kl_t = ds.kl.kl_divergence(z_prior_t_pdf, z_post_t_pdf)\n",
    "            \n",
    "            z_prior.append(z_prior_t)\n",
    "            z_post.append(z_post_t)\n",
    "            kl.append(kl_t)\n",
    "        \n",
    "        z_prior = torch.stack(z_prior) # window_size * vmd_hidden_size\n",
    "        z_post = torch.stack(z_post) # window_size * vmd_hidden_size\n",
    "        self.model.kl = torch.stack(kl).sum(dim=1) # window_size\n",
    "        \n",
    "        self.model.g = nn.Linear(2*vmd_hidden_size, g_size)(\n",
    "            torch.cat([h_s, z_post],dim=1) # TODO: check if X is also to be concatenated as in Eqn 21\n",
    "        )        \n",
    "        self.model.g = nn.Tanh()(self.model.g) #\n",
    "        \n",
    "        y = nn.Linear(g_size, y_size)(self.model.g)\n",
    "        self.model.y = nn.Softmax(dim=1)(y)\n",
    "#         print(\"y:\", self.model.y)\n",
    "\n",
    "        self.model.g_T = self.model.g[-1, :] \n",
    "        # TODO: 1. will g_T be different during training and evaluation?\n",
    "        # TODO: 2. is g_T definitely what you think it is?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e57df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3dbf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ATA(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ATA, self).__init__()\n",
    "        self.model = model\n",
    "        self.alpha = 0.5\n",
    "        \n",
    "        \n",
    "    def forward(self, Y):\n",
    "        linear_i = nn.Linear(g_size, g_size, bias=False)(self.model.g)\n",
    "        linear_i = nn.Tanh()(linear_i) # (window_size, g_size)\n",
    "        w_i = nn.init.xavier_normal_(torch.zeros((g_size, 1))) # (g_size, 1)\n",
    "        v_i = linear_i @ w_i # (window_size, 1)\n",
    "        \n",
    "        linear_d = nn.Linear(g_size, g_size, bias=False)(self.model.g)\n",
    "        linear_d = nn.Tanh()(linear_d)\n",
    "        g_T = self.model.g_T[:, None]\n",
    "        v_d = linear_d @ g_T\n",
    "        \n",
    "        aux_score = torch.mul(v_i, v_d)\n",
    "        aux_score[-1, :] = np.NINF\n",
    "        \n",
    "        v_starred = nn.Softmax(dim=0)(aux_score)\n",
    "        self.model.v_starred = torch.where(torch.isnan(v_starred), torch.tensor(0, dtype=torch.float32), v_starred)\n",
    "        \n",
    "        att_c = self.model.v_starred.T @ self.model.y\n",
    "#         print(att_c.shape, self.model.g_T.shape)\n",
    "        self.model.y_T = nn.Linear(y_size+g_size, y_size)(torch.cat([torch.squeeze(att_c), self.model.g_T]))\n",
    "        self.model.y_T = nn.Softmax(dim=0)(self.model.y_T)  \n",
    "#         print(\"y_T:\", self.model.y_T.shape)\n",
    "        \n",
    "        self.calculate_loss(Y)\n",
    "        \n",
    "        \n",
    "    def calculate_loss(self, Y):\n",
    "#         print(self.model.v_starred)\n",
    "        v_aux = self.alpha * self.model.v_starred\n",
    "        likelihood_aux = torch.sum(torch.mul(Y, torch.log(self.model.y)), dim=1)\n",
    "        \n",
    "        kl_lambda = self.get_kl_lambda()\n",
    "        obj_aux = likelihood_aux - kl_lambda * self.model.kl\n",
    "        \n",
    "        y_T_orig = Y[-1, :]\n",
    "        likelihood_T = torch.sum(torch.mul(y_T_orig, torch.log(self.model.y_T)))\n",
    "        \n",
    "        kl_T = self.model.kl[-1]\n",
    "        obj_T = likelihood_T - kl_lambda * kl_T\n",
    "        \n",
    "        self.model.loss = obj_T + torch.sum(torch.mul(obj_aux, v_aux))\n",
    "        \n",
    "        \n",
    "    def get_kl_lambda(self):\n",
    "        # TODO: implement KL annealing\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab5818f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce2d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9348f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb48f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219b5443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dee512e0",
   "metadata": {},
   "source": [
    "# Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df317cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset/ParlVote/ParlVote_concat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795cc2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speakers[speaker_id: int] = list of tuples (debate_id, speech, vote)\n",
    "speakers = defaultdict(lambda: [])\n",
    "for idx, row in data.iterrows():\n",
    "    speakers[row[\"speaker_id\"]].append([row[\"debate_id\"], row[\"speech\"], row[\"vote\"]])\n",
    "        \n",
    "for k, v in speakers.items():\n",
    "    speakers[k] = sorted(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec6536",
   "metadata": {},
   "source": [
    "## Analysis of speaker sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3bf409",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = 0\n",
    "for usid in data.speaker_id.unique():\n",
    "    mx = max(mx, len(speakers[usid]))\n",
    "print(mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8693da",
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_history_lens = []\n",
    "for sid, each in speakers.items():\n",
    "    speaker_history_lens.append(len(each)) \n",
    "speaker_history_lens.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab5efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "shl = pd.Series(speaker_history_lens)\n",
    "shl.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285925b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(list(filter(lambda x: x >= 4, speaker_history_lens))) / len(speaker_history_lens)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd3ba6",
   "metadata": {},
   "source": [
    "- The above cell shows 90% speakers have at least *window_size*=4 speeches\n",
    "- If we use all data, we need to pad speeches that have (< *window_size*=4) speeches\n",
    "- Decision: Drop these 10% speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eef339",
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = dict(filter(lambda val: len(val[1]) >= 4, speakers.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efb9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 * (len(speakers) / len(speaker_history_lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bdfbf5",
   "metadata": {},
   "source": [
    "## Download Glove-100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2983bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! wget -r -nc https://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ee51f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! head -n1 glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4a9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeds = dict()\n",
    "for line in open(\"nlp.stanford.edu/data/glove.6B.100d.txt\", \"r\"):\n",
    "    line = line.split()\n",
    "    embeds[line[0]] = torch.tensor(list(map(float, line[1:])), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c5552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_vocab = set()\n",
    "# for f in open(\"nlp.stanford.edu/data/glove.6B.100d.txt\", \"r\"):\n",
    "#     glove_vocab.add(f.split()[0])\n",
    "# print(len(glove_vocab))\n",
    "\n",
    "# del glove_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f60ca37",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s: str):\n",
    "    \n",
    "    # Lowercasing\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    ttk = TweetTokenizer()\n",
    "    tokens = ttk.tokenize(s)\n",
    "#     tokens = word_tokenize(s)\n",
    "    \n",
    "    # Stopwords removal\n",
    "    tokens = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    \n",
    "#     # Stemming\n",
    "#     ps = nltk.PorterStemmer()\n",
    "#     tokens = [ps.stem(w) for w in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb28122",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# embed_keys = set(embeds.keys())\n",
    "# ans = 0\n",
    "# i = 0\n",
    "# sids = speakers.keys()\n",
    "# for sid in sids:\n",
    "    \n",
    "#     s = speakers[sid][0][1]\n",
    "#     pre = preprocess(s)\n",
    "#     ans += len(set(pre).difference((set(pre).intersection(embed_keys))))\n",
    "    \n",
    "#     print(\"i\", i, \" done:\", ans)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeds['abhorrent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input1 = embeds[\"drank\"]\n",
    "# input2 = embeds[\"drunk\"]\n",
    "# cos = nn.CosineSimilarity(dim=0)\n",
    "# output = cos(input1, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8b5e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2dae4822",
   "metadata": {},
   "source": [
    "## Create PaddedTensor for input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be61fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_X_Y_padded_tensors(debates):\n",
    "        \"\"\"\n",
    "            X: window_size * max_seq_len * word_embed_size \n",
    "            Y: window_size\n",
    "        \"\"\"\n",
    "        X = []\n",
    "        Y = []\n",
    "        max_tokens = 0\n",
    "        for _, speech, vote in debates:\n",
    "            tokens = preprocess(speech)\n",
    "#             print(\"len preprocessed speech:\", len(tokens))\n",
    "            X.append(torch.stack([embeds.get(token, torch.randn(word_embed_size)) for token in tokens]))\n",
    "            y = torch.zeros(2)\n",
    "            y[vote] = 1. # y = [0, 1] if vote=1 else [1, 0]\n",
    "            Y.append(y)\n",
    "        \n",
    "        X = pad_sequence(X, batch_first=True)\n",
    "        return X, torch.stack(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a317b88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccdaa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = [len(v) for k, v in speakers.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a6bda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_sum = torch.tensor(freq).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13532dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(freq_sum.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24abe58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bdeea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ = 0\n",
    "ans = 0\n",
    "freq_sum = torch.tensor(freq).sum().item()\n",
    "for i in range(len(freq)-1, -1, -1):\n",
    "    if sum_ + (freq[i]) > 0.2 * freq_sum:\n",
    "        break\n",
    "    sum_ += (freq[i])\n",
    "    ans = i\n",
    "ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70a2315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c23c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "for epoch in range(num_epochs):\n",
    "    scnt = 0\n",
    "    done_cnt = 0\n",
    "    for speaker_num, (speaker_id, debates) in enumerate(speakers.items()):\n",
    "        for i in range(window_size, len(debates)+1):\n",
    "            X, Y = get_X_Y_padded_tensors(debates[i-window_size: i]) \n",
    "            model(X, Y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            model.loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            done_cnt += 1\n",
    "            if done_cnt%5==0:\n",
    "                print(\"Speaker num:\", speaker_num, \"\\t\", \"done:\", done_cnt, \"\\t\", \"perc:\", 100*(done_cnt/(0.8*freq_sum)), \"\\n\"+\"-\"*50+\"\\n\")\n",
    "        if scnt > ans:\n",
    "            break\n",
    "        scnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f8f95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
